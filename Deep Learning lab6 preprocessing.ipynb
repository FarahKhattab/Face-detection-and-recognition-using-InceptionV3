{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1g_KOkuDR2qh24Xbb2NSIP5SUfZPrjLy-","timestamp":1713470710824}],"gpuType":"T4","collapsed_sections":["F0BYFJ2aXt3a"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"tdQnGBQEvALT"},"outputs":[],"source":["import cv2\n","import os\n","import numpy as np\n","import tensorflow as tf\n","from google.colab.patches import cv2_imshow\n","from google.colab import files\n","from tensorflow.keras import layers, models\n","from tensorflow.keras.applications.inception_v3 import InceptionV3\n","from tensorflow.keras.optimizers import RMSprop\n","from tensorflow.keras import layers\n","from tensorflow.keras import Model\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from sklearn.model_selection import train_test_split\n","import shutil"]},{"cell_type":"code","source":["# Load the CSV file\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"3bY6mQSDs83-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713473031818,"user_tz":-120,"elapsed":26225,"user":{"displayName":"Farah Khattab","userId":"01272064113233668840"}},"outputId":"ec06a5c3-5955-4f89-e418-ad4c4ae2ed24"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Get the file paths\n","pb_files = '/content/drive/MyDrive/Deep learning lab6/opencv_face_detector_uint8.pb'\n","pbtxt_files = '/content/drive/MyDrive/Deep learning lab6/opencv_face_detector.pbtxt'\n","h5_files = '/content/drive/MyDrive/Deep learning lab6/inception_v3_weights_tf_dim_ordering_tf_kernels.h5'\n","\n"],"metadata":{"id":"z9mX6oJAxBV8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the OpenCV face detector model\n","net = cv2.dnn.readNetFromTensorflow(pb_files, pbtxt_files)\n"],"metadata":{"id":"0nzcu6mlwQ6e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from IPython.display import display, Javascript\n","from google.colab.output import eval_js\n","from base64 import b64decode\n","\n","def take_photo(filename='photo.jpg', quality=0.8):\n","  js = Javascript('''\n","    async function takePhoto(quality) {\n","      const div = document.createElement('div');\n","      const capture = document.createElement('button');\n","      capture.textContent = 'Capture';\n","      div.appendChild(capture);\n","\n","      const video = document.createElement('video');\n","      video.style.display = 'block';\n","      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n","\n","      document.body.appendChild(div);\n","      div.appendChild(video);\n","      video.srcObject = stream;\n","      await video.play();\n","\n","      // Resize the output to fit the video element.\n","      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n","\n","      // Wait for Capture to be clicked.\n","      await new Promise((resolve) => capture.onclick = resolve);\n","\n","      const canvas = document.createElement('canvas');\n","      canvas.width = video.videoWidth;\n","      canvas.height = video.videoHeight;\n","      canvas.getContext('2d').drawImage(video, 0, 0);\n","      stream.getVideoTracks()[0].stop();\n","      div.remove();\n","      return canvas.toDataURL('image/jpeg', quality);\n","    }\n","    ''')\n","  display(js)\n","  data = eval_js('takePhoto({})'.format(quality))\n","  binary = b64decode(data.split(',')[1])\n","  with open(filename, 'wb') as f:\n","    f.write(binary)\n","  return filename"],"metadata":{"id":"z1MliPwb3GFE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to perform face detection on a static photo\n","def detect_faces(frame):\n","    global net\n","\n","    # Perform face detection\n","    blob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300), [104., 177., 123.], False, False)\n","    net.setInput(blob)\n","    detections = net.forward()\n","\n","    # Process the detections\n","    for i in range(0, detections.shape[2]):\n","        # Get the confidence (probability) of the current detection:\n","        confidence = detections[0, 0, i, 2]\n","        # Only consider detections if confidence is greater than a fixed minimum confidence:\n","        if confidence > 0.7:\n","            # Get the coordinates of the current detection:\n","            box = detections[0, 0, i, 3:7] * np.array([frame.shape[1], frame.shape[0], frame.shape[1], frame.shape[0]])\n","            (startX, startY, endX, endY) = box.astype(\"int\")\n","            # Draw the detection and the confidence:\n","            text = \"{:.3f}%\".format(confidence * 100)\n","            y = startY - 10 if startY - 10 > 10 else startY + 10\n","            cv2.rectangle(frame, (startX, startY), (endX, endY), (255, 0, 0), 3)\n","            cv2.putText(frame, text, (startX, y), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n","\n","    # Display the result\n","    cv2_imshow(frame)\n"],"metadata":{"id":"H8gWTqMU4X4j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def detect_and_crop_faces(image_path):\n","    global net\n","    frame = cv2.imread(image_path)\n","\n","    # Perform face detection\n","    blob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300), [104., 177., 123.], False, False)\n","    net.setInput(blob)\n","    detections = net.forward()\n","\n","    # Initialize variables to store the coordinates of the face if found\n","    face_coords = None\n","\n","    # Process the detections\n","    for i in range(0, detections.shape[2]):\n","        # Get the confidence (probability) of the current detection:\n","        confidence = detections[0, 0, i, 2]\n","\n","        # Only consider detections if confidence is greater than a fixed minimum confidence:\n","        if confidence > 0.7:\n","            # Get the coordinates of the current detection:\n","            box = detections[0, 0, i, 3:7] * np.array([frame.shape[1], frame.shape[0], frame.shape[1], frame.shape[0]])\n","            (startX, startY, endX, endY) = box.astype(\"int\")\n","\n","            # Update face_coords with the coordinates of the current detection\n","            face_coords = (startX, startY, endX, endY)\n","\n","            # Break out of the loop as soon as one face is found\n","            break\n","\n","    # If face_coords is None, it means no face was found, so return None\n","    if face_coords is None:\n","        return None\n","\n","    # Otherwise, crop the face region using the coordinates and return it\n","    (startX, startY, endX, endY) = face_coords\n","    face = frame[startY:endY, startX:endX]\n","    return face\n"],"metadata":{"id":"6tbutuq702yP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","from IPython.display import Image\n","from PIL import Image as PIL_Image\n","from io import BytesIO\n","\n","try:\n","    # Capture the photo\n","    filename = take_photo()\n","    print('Saved to {}'.format(filename))\n","\n","    # Load the image using PIL\n","    img_pil = PIL_Image.open(filename)\n","\n","    # Convert PIL Image to NumPy array\n","    img_np = np.array(img_pil)\n","\n","    # Show the image which was just taken.\n","    display(Image(filename))\n","\n","except Exception as err:\n","    # Errors will be thrown if the user does not have a webcam or if they do not\n","    # grant the page permission to access it.\n","    print(str(err))\n"],"metadata":{"id":"UoDpruzp4fUG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["detect_faces(img_np)"],"metadata":{"id":"b3VwF-HA5tXn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load images and labels from Google Drive\n","drive_path = '/content/drive/My Drive/Deep learning lab6/105_classes_pins_dataset'\n","output_path = '/content/drive/My Drive/Deep learning lab6/cropped_images'\n"],"metadata":{"id":"srkaANJItqWY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Cropping Dataset Images"],"metadata":{"id":"F0BYFJ2aXt3a"}},{"cell_type":"code","source":["# Preprocess images and detect faces separately\n","if not os.path.exists(output_path):\n","        os.makedirs(output_path)\n","        print(\"'cropped_images' directory created\")\n","c=1\n","for subdir in os.listdir(drive_path):\n","    print(c)\n","    print(\"\\n\")\n","    c=c+1\n","    subdir_path = os.path.join(drive_path, subdir)\n","    if os.path.isdir(subdir_path):\n","        output_subdir_path = os.path.join(output_path, subdir)\n","        print(output_subdir_path)\n","        if not os.path.exists(output_subdir_path):\n","            os.makedirs(output_subdir_path)\n","        for file in os.listdir(subdir_path):\n","            file_path = os.path.join(subdir_path, file)\n","            if file.endswith('.jpg') or file.endswith('.jpeg') or file.endswith('.png'):\n","                face = detect_and_crop_faces(file_path)\n","                if face is not None:\n","                    output_file_path = os.path.join(output_subdir_path, file)\n","                    cv2.imwrite(output_file_path, face)"],"metadata":{"id":"T4gAbnrQADOy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Spliting Dataset"],"metadata":{"id":"9WxDm6PJX7Bq"}},{"cell_type":"code","source":["# Define paths\n","train_dir = \"/content/drive/My Drive/Deep learning lab6/train\"\n","val_dir = \"/content/drive/My Drive/Deep learning lab6/val\"\n","test_dir = \"/content/drive/My Drive/Deep learning lab6/test\""],"metadata":{"id":"KLD4hlkSUlCP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create directories for subsets\n","os.makedirs(train_dir, exist_ok=True)\n","os.makedirs(val_dir, exist_ok=True)\n","os.makedirs(test_dir, exist_ok=True)"],"metadata":{"id":"GkDL_v7PUo0E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["c=1\n","# Split dataset into training, validation, and testing sets\n","for person_dir in os.listdir(output_path):\n","    print(c)\n","    c=c+1\n","    print(person_dir)\n","\n","    # Create subdirectories in train_dir, val_dir, and test_dir\n","    os.makedirs(os.path.join(train_dir, person_dir), exist_ok=True)\n","    os.makedirs(os.path.join(val_dir, person_dir), exist_ok=True)\n","    os.makedirs(os.path.join(test_dir, person_dir), exist_ok=True)\n","\n","    person_images = os.listdir(os.path.join(output_path, person_dir))\n","    train_images, test_val_images = train_test_split(person_images, test_size=0.3, random_state=42)\n","    val_images, test_images = train_test_split(test_val_images, test_size=0.5, random_state=42)\n","\n","    # Move images to respective directories\n","    for image in train_images:\n","        shutil.copy(os.path.join(output_path, person_dir, image), os.path.join(train_dir, person_dir, image))\n","    for image in val_images:\n","        shutil.copy(os.path.join(output_path, person_dir, image), os.path.join(val_dir, person_dir, image))\n","    for image in test_images:\n","        shutil.copy(os.path.join(output_path, person_dir, image), os.path.join(test_dir, person_dir, image))\n"],"metadata":{"id":"84JtIKF1tSvl"},"execution_count":null,"outputs":[]}]}